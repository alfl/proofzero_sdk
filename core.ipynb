{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProofZero Core SDK\n",
    "\n",
    "A feature parser-hasher client to the Proof Zero matching engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating matches across datasets is difficult. Different data are collected about the same entities (one person enters different phone numbers, or addresses). Data are in different formats -- sometimes just a postal code is stored, sometimes a geo-region, sometimes a street address.\n",
    "\n",
    "However all of these differences in physical data representation refer to the same logical entities.\n",
    "\n",
    "The Proof Zero SDK generates pseudokeys that match entities across data sets containing different physical data representations. These pseudokeys establish a primary/foreign key relationship between incomplete, incorrect, and unclean data sets. A measure of match strength is also created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to join two datasets using pseudokeys we first `load` our data, and then:\n",
    "\n",
    "1. `tokenize` it -- decompose our data into standard formats,\n",
    "2. `index` it -- recompose our tokens into cryptographic indexes,\n",
    "3. `match` it -- compare indexed data sets to generate matches and measure match quality.\n",
    "\n",
    "This is the TIM: `tokenize`, `index`, `match` workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security and data privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All SDK features run locally, so data remain within control of the user. match optionally allows use of Proof Zero's compute cluster and matching models to generate matches more quickly and with higher accuracy.\n",
    "\n",
    "By using IndexFrame objects data remain in the control of the local system, even when using Proof Zero's compute cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exported types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide several lightweight types that track progress through the TIM workflow:\n",
    "\n",
    "* `TokenFrame` is a `pandas` DataFrame that has been tokenized.\n",
    "* `IndexFrame` is a `TokenFrame` that has been hashed/indexed.\n",
    "* `MatchFrame` is a match between two `IndexFrame`s.\n",
    "\n",
    "These types help systems and analysts keep track of where a given dataset is in the tokenize, index, match (TIM) workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "def load(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience function for loading files using `pandas`. Full version supports XLSX, etc.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable, NewType\n",
    "\n",
    "# The `TokenFrame` type is a wrapper around a `pandas` DataFrame that represents a DataFrame that has been tokenized using the `tokenize` function.\n",
    "TokenFrame = NewType('TokenFrame', pd.DataFrame)\n",
    "\n",
    "def tokenize(\n",
    "    df: pd.DataFrame, schema: dict, suffix_delim: str = \"\"\n",
    ") -> TokenFrame:\n",
    "    \"\"\"\n",
    "    Takes a `pandas` DataFrame and a schema. The schema is a `dict` that maps columns in the DataFrame to a list of parsers that are executed in order.\n",
    "    \n",
    "    Denormalizes the passed DataFrame by applying the parsers in the schema.\n",
    "    \n",
    "    Returns the denormalized DataFrame.\n",
    "    \"\"\"\n",
    "    def map_schema(map_row, map_schema, map_delim):\n",
    "        results = [\n",
    "            functools.reduce(\n",
    "                lambda data, fxn: fxn(data), map_schema[i], map_row[i]\n",
    "            )\n",
    "            for i in map_row.index\n",
    "        ]\n",
    "        indicies = [\n",
    "            # Allow parsers to return a named component and use that name to index\n",
    "            # our new columns, else use the index number as a string.\n",
    "            map_delim.join(\n",
    "                [map_row.index[i], v[0] if isinstance(v, tuple) else str(j)]\n",
    "            )\n",
    "            for i, u in enumerate(map_row.index)\n",
    "            for j, v in enumerate(results[i])\n",
    "        ]\n",
    "        series = pd.Series(\n",
    "            data=list(itertools.chain(*results)), index=indicies\n",
    "        )\n",
    "        return series\n",
    "\n",
    "    return pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            df.apply(\n",
    "                map_schema,\n",
    "                axis=1,\n",
    "                args=(schema, suffix_delim),\n",
    "                result_type=\"expand\",\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from proofzero_sdk.util import sha2\n",
    "\n",
    "# The `IndexFrame` type is a wrapper around a `TokenFrame` (ie, a `pandas` DataFrame) that represents a tokenized DataFrame that has been indexed using the `index` function.\n",
    "IndexFrame = NewType('IndexFrame', TokenFrame)\n",
    "\n",
    "def index(\n",
    "    df: TokenFrame, schema: dict, hasher: Callable = sha2\n",
    ") -> IndexFrame:\n",
    "    \"\"\"\n",
    "    Tokenizes the passed DataFrame using `tokenize`, then applies a hash function to the tokenized features.\n",
    "    \n",
    "    The default hash function is `sha2`, from the Proof Zero utility SDK.\n",
    "    \"\"\"\n",
    "    return tokenize(df, schema).applymap(hasher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# The `MatchFrame` type is a wrapper around a `IndexFrame` (ie, a `TokenFrame` and ultimately a `pandas` DataFrame) that represents an indexed DataFrame that has been matched using the `match` function.\n",
    "MatchFrame = NewType('MatchFrame', IndexFrame)\n",
    "\n",
    "def match(\n",
    "    index_0: IndexFrame, index_1: IndexFrame, network: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Discover matches between the two passed DataFrames. If `network` is set to `True` the indexed data are matched using Proof Zero's cluster.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
