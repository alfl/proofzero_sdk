{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProofZero SDK\n",
    "\n",
    "> A feature parser-hasher client to the Proof Zero matching engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "def load(filename):\n",
    "    \"\"\"\n",
    "    Convenience function for loading files using `pandas`. Full version supports XLSX, etc.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize(\n",
    "    df: pd.DataFrame, schema: dict, suffix_delim: str = \"\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a `pandas` DataFrame and a schema. The schema is a `dict` that maps columns in the DataFrame to a list of parsers that are executed in order.\n",
    "    \n",
    "    Denormalizes the passed DataFrame by applying the parsers in the schema.\n",
    "    \n",
    "    Returns the denormalized DataFrame.\n",
    "    \"\"\"\n",
    "    def map_schema(map_row, map_schema, map_delim):\n",
    "        results = [\n",
    "            functools.reduce(\n",
    "                lambda data, fxn: fxn(data), map_schema[i], map_row[i]\n",
    "            )\n",
    "            for i in map_row.index\n",
    "        ]\n",
    "        indicies = [\n",
    "            # Allow parsers to return a named component and use that name to index\n",
    "            # our new columns, else use the index number as a string.\n",
    "            map_delim.join(\n",
    "                [map_row.index[i], v[0] if isinstance(v, tuple) else str(j)]\n",
    "            )\n",
    "            for i, u in enumerate(map_row.index)\n",
    "            for j, v in enumerate(results[i])\n",
    "        ]\n",
    "        series = pd.Series(\n",
    "            data=list(itertools.chain(*results)), index=indicies\n",
    "        )\n",
    "        return series\n",
    "\n",
    "    return pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            df.apply(\n",
    "                map_schema,\n",
    "                axis=1,\n",
    "                args=(schema, suffix_delim),\n",
    "                result_type=\"expand\",\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable\n",
    "def index(\n",
    "    df: pd.DataFrame, schema: dict, hasher: Callable = None # sha2.apply\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tokenizes the passed DataFrame using `tokenize`, then applies a hash function to the tokenized features.\n",
    "    \"\"\"\n",
    "    return tokenize(df, schema).applymap(hasher)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
